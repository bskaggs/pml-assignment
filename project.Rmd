---
title: "Do It Right: Evaluating Proper Excercise Form"

output: html_document
---

In this project, we build a machine learning algorithm to predict activity quality from activity monitors.  More information about the data can be found at [Groupware at LES](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises).

We first load necessary libraries, including one for parallel computation, and then set our random seed for reproducibility of our results.  We then download our labeled data.

```{r, message=FALSE}
library(caret); library(data.table); library(doMC)
registerDoMC(cores = 5)
set.seed(12345)

dload <- function(file, root = "https://d396qusza40orc.cloudfront.net/predmachlearn") {
  if (!file.exists(file)) {
    download.file(paste(root, file, sep = "/"), destfile = file, method = 'curl')
  }
  read.csv(file)
}

labeled <- dload("pml-training.csv")
```

We first partition the data into approximately 75% training, and 25% testing.

```{r}
inTrain <- createDataPartition(y = labeled$classe, p = 0.75 ,list = FALSE)
training <- labeled[inTrain,]
testing <- labeled[-inTrain,]
```

Lets check for fields with missing values.

```{r}
table((colSums(training == "", na.rm = TRUE) + colSums(is.na(training))) / nrow(training))
```

Wow, 100 columns are almost completely empty, but the rest have values for every row.  Let's just use those columns.

```{r}
training <- training[,colSums(is.na(training)) == 0]
training <- training[,apply(training != "", 2, all)]
```

The first 7 columns all have to do with the mechanics of the data collection such as the user_name and timestamp fields.  We will ignore them because they will not be useful in future data from other people. (Do note that if you were to build a classifier using these fields, you get very close to 100% accuracy, since the data was generated by each person doing each type of activity at a different time!)

```{r}
training <- training[,8:ncol(training)]
```

That leaves us with the following fields for our predictions:
```{r}
names(training)
```

We fit a random forest model to the training data, and see how long it takes to train it:  

```{r, message=FALSE}
system.time(modFit <- train(classe ~ ., data = training, method="rf", prox=TRUE))
```

We will predict with our model on the held out test set to build a confusion matrix:

```{r}
testing$predictedClasse <- predict(modFit, testing)

confusionMatrix(table(testing$predictedClasse,  testing$classe))
```

Although mentioned above, we can explicitly calculate the out-of-sample accuracy:

```{r}
accuracy <- sum(testing$predictedClasse == testing$classe) / dim(testing)[1]
accuracy
```

We expect the out-of-sample error, as estimated via cross validation, to be 1 - accuracy = `r 1 - accuracy`.

Let's look at the important variables in our model, as calcuated by the out-of-bag data:

```{r}
varImp(modFit)
```

Now, we predict on our data to submit, and write them to files using the code provided in the submission instructions:

```{r}
submission <- dload("pml-testing.csv")

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

answers <- predict(modFit, submission)
pml_write_files(answers)
```

All that is left now is to submit and see how we do!

